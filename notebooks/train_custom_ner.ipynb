{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom NER model training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom parameters\n",
    "\n",
    "Customize your model training changing the default parameters:\n",
    "\n",
    "- __Pipeline settings__\n",
    "  - __verbose__: Boolean, print steps and partial results\n",
    "  - __p_seed__: Integer, used for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "verbose = True\n",
    "p_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Training set__\n",
    "  - __train_path__: String, path to annotated CSV\n",
    "  - __num_samples__: Integer\n",
    "  - __split_train__: Float, must be 0 < n < 1 . For example, 0.7 means 70% used for training the model.\n",
    "  - __split_validation__: Float, using the rest from split_train, keep portion for inference. For example, 0.1 means 10% used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data set parameters\n",
    "p_ner_vendor = False\n",
    "p_ner_product = False\n",
    "p_ner_version = True\n",
    "\n",
    "train_path = \"../datasets/trainsets/train_cpener_vers_500k_wgh42.csv.gz\"\n",
    "num_samples = 100000\n",
    "split_train = 0.7\n",
    "split_validation = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Model settings__\n",
    "  - __pretrained_token_name__: \"Neurona/cpener-test\" # distilbert-base-uncased, distilbert-base-cased, bert-base-NER, bert-large-NER, flair/ner-english-ontonotes-fast, Neurona/cpener-test\n",
    "  - __pretrained_model_name__: \"Neurona/cpener-test\" # distilbert-base-uncased, distilbert-base-cased, bert-base-NER, bert-large-NER, flair/ner-english-ontonotes-fast, Neurona/cpener-test\n",
    "  - __num_epochs__: 10\n",
    "  - __num_decay__: 0.01\n",
    "  - __token_truncation__: False\n",
    "  - __train_learning_rate__: 2e-5\n",
    "  - __train_patience__: 8\n",
    "  - __train_batch_size__: 32\n",
    "  - __eval_batch_size__: 32\n",
    "  - __train_logging_steps__: 100\n",
    "  - __save_model_name__: \"cpener_vpv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "pretrained_token_name = \"distilbert-base-uncased\" # distilbert-base-uncased, distilbert-base-cased, bert-base-NER, bert-large-NER, flair/ner-english-ontonotes-fast, Neurona/cpener-test\n",
    "pretrained_model_name = \"distilbert-base-uncased\" # distilbert-base-uncased, distilbert-base-cased, bert-base-NER, bert-large-NER, flair/ner-english-ontonotes-fast, Neurona/cpener-test\n",
    "num_epochs = 20\n",
    "num_decay = 0.01\n",
    "token_truncation = False\n",
    "train_learning_rate = 2e-5\n",
    "train_patience = 5\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "train_logging_steps = 100\n",
    "save_model_path = \"../models\"\n",
    "\n",
    "# Inference validation\n",
    "results_path = \"../datasets/results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (p_ner_vendor and p_ner_product and p_ner_version):\n",
    "    str_ner = \"vpv\"\n",
    "elif (p_ner_vendor and p_ner_product and not (p_ner_version)):\n",
    "    str_ner = \"vp\"\n",
    "elif (not (p_ner_vendor) and p_ner_product and p_ner_version):\n",
    "    str_ner = \"pv\"\n",
    "elif (p_ner_vendor and not (p_ner_product) and p_ner_version):\n",
    "    str_ner = \"vv\"\n",
    "elif (p_ner_vendor and not (p_ner_product) and not (p_ner_version)):\n",
    "    str_ner = \"vend\"\n",
    "elif (not (p_ner_vendor) and p_ner_product and not (p_ner_version)):\n",
    "    str_ner = \"prod\"\n",
    "elif (not (p_ner_vendor) and (not p_ner_product) and p_ner_version):\n",
    "    str_ner = \"vers\"\n",
    "else:\n",
    "    str_ner = \"nan\"\n",
    "\n",
    "results_path = f\"{results_path}/ner_predictions_{str_ner}.csv\"\n",
    "results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_name = f\"{save_model_path}/db_cpener_{str_ner}\"\n",
    "save_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Model\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import pipeline\n",
    "\n",
    "# Inference\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set Seed\n",
    "np.random.seed(p_seed)\n",
    "\n",
    "# Set Start Time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_with_entities(raw_text: str):\n",
    "    # split the text by spaces only if the space does not occur between square brackets\n",
    "    # we do not want to split \"multi-word\" entity value yet\n",
    "    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n",
    "\n",
    "    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n",
    "    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n",
    "    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M)\n",
    "\n",
    "    tokens_with_entities = []\n",
    "\n",
    "    for raw_token in raw_tokens:\n",
    "        match = entity_value_pattern_compiled.match(raw_token)\n",
    "        if match:\n",
    "            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n",
    "\n",
    "            # we prefix the name of entity differently\n",
    "            # B- indicates beginning of an entity\n",
    "            # I- indicates the token is not a new entity itself but rather a part of existing one\n",
    "            for i, raw_entity_token in enumerate(re.split(\"\\s\", raw_entity_value)):\n",
    "                entity_prefix = \"B\" if i == 0 else \"I\"\n",
    "                entity_name = f\"{entity_prefix}-{raw_entity_name}\"\n",
    "                tokens_with_entities.append((raw_entity_token, entity_name))\n",
    "        else:\n",
    "            tokens_with_entities.append((raw_token, \"O\"))\n",
    "\n",
    "    return tokens_with_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ner_out(out, ent_vend, ent_prod, ent_vers):\n",
    "    if (ent_vend):\n",
    "        ner_vendor = \"\"\n",
    "        scr_vendor = 0.0\n",
    "\n",
    "    if (ent_prod):\n",
    "        ner_product = \"\"\n",
    "        scr_product = 0.0\n",
    "\n",
    "    if (ent_vers):\n",
    "        ner_version = \"\"\n",
    "        scr_version = 0.0\n",
    "\n",
    "    if (out == []):\n",
    "        if (ent_vend and ent_prod and ent_vers):\n",
    "            return({\"ner_vendor\": ner_vendor,\n",
    "                    \"scr_vendor\": scr_vendor,\n",
    "                    \"ner_product\": ner_product,\n",
    "                    \"scr_product\": scr_product,\n",
    "                    \"ner_version\": ner_version,\n",
    "                    \"scr_version\": scr_version})\n",
    "        elif (ent_vend and ent_prod and not(ent_vers)):\n",
    "            return({\"ner_vendor\": ner_vendor,\n",
    "                    \"scr_vendor\": scr_vendor,\n",
    "                    \"ner_product\": ner_product,\n",
    "                    \"scr_product\": scr_product})\n",
    "        elif (not(ent_vend) and ent_prod and ent_vers):\n",
    "            return({\"ner_product\": ner_product,\n",
    "                    \"scr_product\": scr_product,\n",
    "                    \"ner_version\": ner_version,\n",
    "                    \"scr_version\": scr_version})\n",
    "        elif (ent_vend and not(ent_prod) and ent_vers):\n",
    "            return({\"ner_vendor\": ner_vendor,\n",
    "                    \"scr_vendor\": scr_vendor,\n",
    "                    \"ner_version\": ner_version,\n",
    "                    \"scr_version\": scr_version})\n",
    "        elif (ent_vend and not(ent_prod) and not(ent_vers)):\n",
    "            return({\"ner_vendor\": ner_vendor,\n",
    "                    \"scr_vendor\": scr_vendor})\n",
    "        elif (not(ent_vend) and ent_prod and not(ent_vers)):\n",
    "            return({\"ner_product\": ner_product,\n",
    "                    \"scr_product\": scr_product})\n",
    "        elif (not(ent_vend) and not(ent_prod) and ent_vers):\n",
    "            return({\"ner_version\": ner_version,\n",
    "                    \"scr_version\": scr_version})\n",
    "        else:\n",
    "            return({})\n",
    "    \n",
    "    df_ner = pd.DataFrame.from_dict(out)  \n",
    "    \n",
    "    if ('vendor' in df_ner['entity_group'].values):\n",
    "        ner_vendor = df_ner[df_ner['entity_group'] == \"vendor\"].groupby(\"entity_group\").agg({'word': ' '.join}).word.iloc[0]\n",
    "        ner_vendor = re.sub(r'([^ ]+) ([^\\d|^\\w]) ([^ ]+)', \"\\\\1\\\\2\\\\3\", ner_vendor)\n",
    "        scr_vendor = df_ner[df_ner['entity_group'] == \"vendor\"].groupby(\"entity_group\").mean(\"score\").score.iloc[0]\n",
    "    if ('product' in df_ner['entity_group'].values):\n",
    "        ner_product = df_ner[df_ner['entity_group'] == \"product\"] .groupby(\"entity_group\").agg({'word': ' '.join}).word.iloc[0]\n",
    "        ner_product = re.sub(r'([^ ]+) ([^\\d|^\\w]) ([^ ]+)', \"\\\\1\\\\2\\\\3\", ner_product)\n",
    "        scr_product = df_ner[df_ner['entity_group'] == \"product\"] .groupby(\"entity_group\").mean(\"score\").score.iloc[0]\n",
    "    if ('version' in df_ner['entity_group'].values):\n",
    "        ner_version = df_ner[df_ner['entity_group'] == \"version\"] .groupby(\"entity_group\").agg({'word': '.'.join}).word.iloc[0]\n",
    "        ner_version = re.sub(r'\\.+', \".\", ner_version)\n",
    "        scr_version = df_ner[df_ner['entity_group'] == \"version\"] .groupby(\"entity_group\").mean(\"score\").score.iloc[0]\n",
    "    \n",
    "    if (ent_vend and ent_prod and ent_vers):\n",
    "        return({\"ner_vendor\": ner_vendor,\n",
    "                \"scr_vendor\": scr_vendor,\n",
    "                \"ner_product\": ner_product,\n",
    "                \"scr_product\": scr_product,\n",
    "                \"ner_version\": ner_version,\n",
    "                \"scr_version\": scr_version})\n",
    "    elif (ent_vend and ent_prod and not(ent_vers)):\n",
    "        return({\"ner_vendor\": ner_vendor,\n",
    "                \"scr_vendor\": scr_vendor,\n",
    "                \"ner_product\": ner_product,\n",
    "                \"scr_product\": scr_product})\n",
    "    elif (not(ent_vend) and ent_prod and ent_vers):\n",
    "        return({\"ner_product\": ner_product,\n",
    "                \"scr_product\": scr_product,\n",
    "                \"ner_version\": ner_version,\n",
    "                \"scr_version\": scr_version})\n",
    "    elif (ent_vend and not(ent_prod) and ent_vers):\n",
    "        return({\"ner_vendor\": ner_vendor,\n",
    "                \"scr_vendor\": scr_vendor,\n",
    "                \"ner_version\": ner_version,\n",
    "                \"scr_version\": scr_version})\n",
    "    elif (ent_vend and not(ent_prod) and not(ent_vers)):\n",
    "        return({\"ner_vendor\": ner_vendor,\n",
    "                \"scr_vendor\": scr_vendor})\n",
    "    elif (not(ent_vend) and ent_prod and not(ent_vers)):\n",
    "        return({\"ner_product\": ner_product,\n",
    "                \"scr_product\": scr_product})\n",
    "    elif (not(ent_vend) and not(ent_prod) and ent_vers):\n",
    "        return({\"ner_version\": ner_version,\n",
    "                \"scr_version\": scr_version})\n",
    "    else:\n",
    "        return({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hackvers(row):\n",
    "    vers = [i for i in row['title'].split() if i.startswith(row['ner_version'])]\n",
    "    return ''.join(vers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Class for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataMaker:\n",
    "    def __init__(self, texts):\n",
    "        self.unique_entities = []\n",
    "        self.processed_texts = []\n",
    "\n",
    "        temp_processed_texts = []\n",
    "        for text in texts:\n",
    "            tokens_with_entities = get_tokens_with_entities(text)\n",
    "            for _, ent in tokens_with_entities:\n",
    "                if ent not in self.unique_entities:\n",
    "                    self.unique_entities.append(ent)\n",
    "            temp_processed_texts.append(tokens_with_entities)\n",
    "\n",
    "        self.unique_entities.sort(key=lambda ent: ent if ent != \"O\" else \"\")\n",
    "\n",
    "        for tokens_with_entities in temp_processed_texts:\n",
    "            self.processed_texts.append([(t, self.unique_entities.index(ent)) for t, ent in tokens_with_entities])\n",
    "\n",
    "    @property\n",
    "    def id2label(self):\n",
    "        return dict(enumerate(self.unique_entities))\n",
    "\n",
    "    @property\n",
    "    def label2id(self):\n",
    "        return {v:k for k, v in self.id2label.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def _process_tokens_for_one_text(id, tokens_with_encoded_entities):\n",
    "            ner_tags = []\n",
    "            tokens = []\n",
    "            for t, ent in tokens_with_encoded_entities:\n",
    "                ner_tags.append(ent)\n",
    "                tokens.append(t)\n",
    "\n",
    "            return {\n",
    "                \"id\": id,\n",
    "                \"ner_tags\": ner_tags,\n",
    "                \"tokens\": tokens\n",
    "            }\n",
    "\n",
    "        tokens_with_encoded_entities = self.processed_texts[idx]\n",
    "        if isinstance(idx, int):\n",
    "            return _process_tokens_for_one_text(idx, tokens_with_encoded_entities)\n",
    "        else:\n",
    "            return [_process_tokens_for_one_text(i+idx.start, tee) for i, tee in enumerate(tokens_with_encoded_entities)]\n",
    "\n",
    "    def as_hf_dataset(self, tokenizer):\n",
    "        from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
    "        def tokenize_and_align_labels(examples):\n",
    "            tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=token_truncation, is_split_into_words=True)\n",
    "\n",
    "            labels = []\n",
    "            for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "                word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "                previous_word_idx = None\n",
    "                label_ids = []\n",
    "                for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                        label_ids.append(label[word_idx])\n",
    "                    else:\n",
    "                        label_ids.append(-100)\n",
    "                    previous_word_idx = word_idx\n",
    "                labels.append(label_ids)\n",
    "\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "            return tokenized_inputs\n",
    "\n",
    "        ids, ner_tags, tokens = [], [], []\n",
    "        for i, pt in enumerate(self.processed_texts):\n",
    "            ids.append(i)\n",
    "            pt_tokens,pt_tags = list(zip(*pt))\n",
    "            ner_tags.append(pt_tags)\n",
    "            tokens.append(pt_tokens)\n",
    "        data = {\n",
    "            \"id\": ids,\n",
    "            \"ner_tags\": ner_tags,\n",
    "            \"tokens\": tokens\n",
    "        }\n",
    "        features = Features({\n",
    "            \"tokens\": Sequence(Value(\"string\")),\n",
    "            \"ner_tags\": Sequence(ClassLabel(names=dm.unique_entities)),\n",
    "            \"id\": Value(\"int32\")\n",
    "        })\n",
    "        ds = Dataset.from_dict(data, features)\n",
    "        tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
    "        return tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load annotated data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_path)\n",
    "df['annotated'] = df['annotated'].astype(str) + '.'\n",
    "\n",
    "if (verbose):\n",
    "    # time taken to read data\n",
    "    e_time = time.time()\n",
    "    print(\"Read without chunks: \", (e_time-start_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select custom sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.loc[np.random.choice(df.index, num_samples)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Train, Test and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, validate, test = train_validate_test_split(df_sample, train_percent=split_train, \n",
    "                                                  validate_percent=split_validation, seed=p_seed)\n",
    "\n",
    "train_text = train.annotated.to_list()\n",
    "test_text = test.annotated.to_list()\n",
    "validate_text = validate.annotated.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (verbose):\n",
    "    print(\"Train annotated sample: \" + str(get_tokens_with_entities(train_text[0])))\n",
    "    print(\"Test annotated sample: \" + str(get_tokens_with_entities(test_text[0])))\n",
    "    print(\"Validation annotated sample: \" + str(get_tokens_with_entities(validate_text[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create NER Data Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training NER Data Object\n",
    "dm = NERDataMaker(train_text)\n",
    "if (verbose):\n",
    "    print(\"TRAIN NER DATA OBJECTS\")\n",
    "    print(f\"  - total examples = {len(dm)}\")\n",
    "    print(f\"  - labels = {dm.id2label}\")\n",
    "    print(f\"  - Examples = {dm[0:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NER Data Object\n",
    "dm_test = NERDataMaker(test_text)\n",
    "if (verbose):\n",
    "    print(\"TEST NER DATA OBJECTS\")\n",
    "    print(f\"  - total examples = {len(dm_test)}\")\n",
    "    print(f\"  - labels = {dm_test.id2label}\")\n",
    "    print(f\"  - Examples = {dm_test[0:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NER Data Object\n",
    "dm_validate = NERDataMaker(validate_text)\n",
    "if (verbose):\n",
    "    print(\"VALIDATE NER DATA OBJECTS\")\n",
    "    print(f\"  - total examples = {len(dm_validate)}\")\n",
    "    print(f\"  - labels = {dm_validate.id2label}\")\n",
    "    print(f\"  - Examples = {dm_validate[0:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (verbose):\n",
    "    print(\"LABELS SUMMARY:\")\n",
    "    print(f\"  - Train labels = {dm.id2label}\")\n",
    "    print(f\"  - Test labels = {dm_test.id2label}\")\n",
    "    print(f\"  - Validation labels = {dm_validate.id2label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom NER model\n",
    "For this demo, I’ll use distilbert-base-uncased model. The dm object contains few properties which we pass to the AutoModelForTokenClassification.from_pretrained method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_token_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dm.as_hf_dataset(tokenizer=tokenizer)\n",
    "test_ds = dm_test.as_hf_dataset(tokenizer=tokenizer)\n",
    "validate_ds = dm_validate.as_hf_dataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(pretrained_model_name, num_labels=len(dm.unique_entities), id2label=dm.id2label, label2id=dm.label2id, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/results\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    logging_first_step=True,\n",
    "    # save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=train_logging_steps,\n",
    "    learning_rate=train_learning_rate,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=num_decay,\n",
    "    seed = p_seed,\n",
    "    data_seed = p_seed,\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds, \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure training callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=train_patience))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.add_callback(TensorBoardCallback())\n",
    "tensorboard_sm = SummaryWriter(log_dir=training_args.logging_dir)\n",
    "tensorboard_cb = TensorBoardCallback(tensorboard_sm)\n",
    "trainer.add_callback(tensorboard_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (verbose):\n",
    "    print(trainer.model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train custom NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(save_model_name)\n",
    "tokenizer.save_pretrained(save_model_name + \"/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define inference pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"max\", device=0) # pass device=0 if using gpu\n",
    "\n",
    "def predict_cpe_ner(df, col_name):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    out_ner = []\n",
    "    for out in pipe(KeyDataset(dataset, col_name), batch_size=8):\n",
    "        i = process_ner_out(out, p_ner_vendor, p_ner_product, p_ner_version)\n",
    "        out_ner.append(i)\n",
    "\n",
    "    df_predict = pd.DataFrame.from_dict(out_ner)\n",
    "    \n",
    "    return df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show data for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (verbose):\n",
    "    display(validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict entities using custom NER model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = predict_cpe_ner(validate, \"title\")\n",
    "if (verbose):\n",
    "    display(df_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply hack for version entity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_result = pd.concat([validate.loc[:,[i for i in validate.columns if not (i.startswith('annotated') or i.startswith('cpe'))]].reset_index(drop=True), df_predict], axis=1)\n",
    "if (\"ner_version\" in df_result.columns):\n",
    "    df_result['ner_version_raw'] = df_result['ner_version']\n",
    "    df_result['ner_version'] = df_result.apply(hackvers, axis=1)\n",
    "if (verbose):\n",
    "    display(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv(results_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
